---
title: "`micromatch` package:"
subtitle: "Utilities and functions for statistical matching"
author: "Ines Garmendia"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{micromatch package}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc}
---

About this document
===================

<p align="justify">This is the main vignette for `micromatch` package. This package provides a set of utilities to ease the task of statistically matching independent microdata files, with a focus to official statistics. 

The main methods in `micromatch` are described in two books (see [1] and [2]), and are a result of two Eurostat projects in Data Integration and Statistical Matching (see [3] and [4]). `micromatch` heavily relies on `StatMatch`, the R package that implements the methods in those sources.</p>

<p align="justify">This document has two main parts. In the first chapter you will find an overview of the main concepts in statistical matching methodology. In the second chapter you will see how `micromatch` can be used in practice, to solve diverse matching tasks.</p>

<p align="justify">`micromatch` package also provides vignettes for specific examples with real data. These additional documents can be found in the package documentation.</p>

Fundamentals of Statistical Matching
====================================

> Statistical matching provides a methodology to explore ways for producing combined analyses or indicators for independent surveys referred to the same population of interest, from data with to distinct observations and stored in separate files, but sharing a common block of information

<p align="justify"><strong>Statistical matching</strong> (also known as data fusion, data merging or synthetic matching) is set of techniques for providing joint information on variables or indicators collected through multiple sources, usually, surveys drawn from the same population of interest. The potential benefits of this approach lie in the possibility to enhance the complementary use and analytical potential of existing data sources. ([5] A. Leulescu & M. Agafitei, 2013).</p>

<p align="justify">Statistical matching has been widely used in market research, to link consumer behavior data and media consumption data.</p>

<p align="justify">In official statistics, it can be used to link different aspects that are usually studied separately for the same target population (i.e. the inhabitants in a country or a particular geographic area). A unique questionnaire covering all aspects such as population health, income, consumption, labour market, social capital... is seldom conceived: such a questionnaire would be too long, leading to a higher response burden, and to poor quality.</p>

<p align="justify">On the contrary, a separate survey is usually conducted to study each specific aspect of the population, the drawback being that the responses will eventually lie in separate files. Statistical matching tries to overcome this limitation, by making use of shared information in order to infer some type of "new" knowledge about aspects measured by independent surveys.</p>

The starting point (the input)
------------------------------

> The basic assumption is that the number of individuals or units in both samples (i.e., the overlap) is negligible. In fact, the fundamental difference with respect to other methods such as "record linkage" is that in the latter, we have identical units and we wish to find a correspondence between them to link the files. 
In statistical matching, we "know" the units are different, but we "wish" to find similar ones.

<p align="justify">Consider two independent surveys conducted on the same population of interest, each of which produces measures regarding a specific field (e.g. living styles and consumer behavior).</p> 

<p align="justify">The surveys share a block of variables (sociodemographic variables such as the age, sex, or social status). When putting all the observations surveys together, a particular missing data pattern emerges due to the non-observed values (i.e. answers we don't have in one survey just because they correspond to the other, independent survey), see Fig 1.</p>

<div align="center"><img src="./fig1.png"><figcaption>Fig 1. The starting point: a block of common variables (Z) and two block of specific, non-jointly-observed, variables (X and Y)</figcaption></div>

<p align="justify">The aim is to obtain integrated analyses or results relating the non-jointly-observed variables (blocks X and Y in the figure), and to achieve this we need to make use of the common information between the files (block Z) in some efficient and reliable way.</p>

The results (the output)
------------------------

<p align="justify">After matching will typically obtain one of these results:</p>

* a synthetic file containing full information on the variables and all units from both sources. This enhanced dataset can be used later to make combined statistical analyses.

* particular estimates regarding variables living in separate files. The user might wish to estimate a contingency table or correlation coefficient, or any parameter of interest regarding variables in separate files. 

The former is named the **micro** approach. The latter is the **macro** approach.

The matching process
--------------------

<p align="justify">Regardless of the matching method itself —that is, the computational method by means of which we will produce a synthetic file (in the micro case) or estimations for certain parameters of the joint distributions (in the macro case)—, the matching task involves a series of <strong>pre-processing steps</strong> that have to be tackled in practice:</p>

1. The choice of target variables (X and Y), i.e. the variables observed in distinct surveys.

2. Identification of the variables shared by the sources, and the study of the degree of coherence taking into account not only the wording of questions (which can be different, leading to non-agreeable measures), but also the marginal distributions observed in the data files. Variables that fail to show a minimum degree of coherence must be discarded. This step can be time-consuming but can also the key for a successful matching. 

3. Possibly, discarding further variables that are not predictive (i.e. are not related to) for the target variables, o are redundant with others. This can be of a relevant step, depending on the computational method. 

4. The choice of a matching framework (parametric, non-parametric, mixed) in a specific setting (micro or macro), and applying the corresponding matching/imputation/estimation algorithm. This algorithm will make use of the chosen subset of shared variables in steps 2 and 3 (namely, the _common matching variables_) to relate target variables fixed in step 1 (the _specific variables_).

5. A validation of results.

Using `micromatch`
=================

<p align="justify">In this chapter you will find how `micromatch` may be used to tackle a specific matching task with real data, and how this package relates to —and is based on— other packages.</p>

Relation to other packages
---------------------------

<p align="justify">As opposed to other packages such as `StatMatch` or `mice` that provide sophisticated functions to solve different statistical matching tasks, `micromatch` does not offer new algorithms for matching but, rather, provides a context the matching task is made easier, independently of the chosen methodology.</p>

<p align="justify">Specifically, `micromatch` does not offer new functions to solve the statistical matching problem, but a <strong>framework</strong> where the main methods implemented in other packages become related.</p>

<p align="justify">To achieve this integration, `micromatch` uses S4 classes and methods so that the user will start defining particular attributes of the data related to the statistical matching context. In particular, each step in the matching process has its implementation (or definition) in `micromatch`.</p>

A simple example
----------------

<p align="justify">To illustrate the use of `micromatch` we will be using data frames data `samp.A` and `samp.B` included in `StatMatch` package. These examples provide some artificial data simulating typical variables present in the European Union Statistics on Income and Living Conditions Survey (EU-SILC).</p>

```{r loadStatMatch, warning=FALSE, message=FALSE}
library(StatMatch)
data(samp.A) #loads data into workspace
data(samp.B) #loads data to workspace
str(samp.A)
str(samp.B)
```

The independent sources `samp.A` and `samp.B`, separately contain:

* a shared block of variables: 

    + `HH.P.id`: unit identifier
    + `area5` and `urb`: geographic variables
    + `hsize` and `hsize5`: family size (numeric and categorized)
    + `age` and `c.age`: age (numeric and categorized)
    + `sex`: gender
    + `marital`: marital status
    + `edu7`: education level

* one specific variable in each of the files:

    + in file `samp.A`: `n.income` and `c.neti`, net personal income (numeric and categorized, thousand of euros)
    
    + in file `samp.B`: `labour5`, the person's self-defined economic status.

* a weight variable,    `ww`, with the same name in both files

For more information on these data files please refer to `StatMatch` package documentation.

What follows is an illustration of how the matching task can be tackled with `micromatch`, in each of the mentioned steps:

#### Step 1: 

The specific (target) variables are the income and the labour status, and it is advisable to store their name. For this example we will use the categorical version of variable income, `c.neti`.

```{r}
varesp_A <- "c.neti" # specific variable in file samp.A
varesp_B <- "labour5" # specific variable in file samp.B
```

#### Step 2: 

<p align="justify">The shared variables are the remaining variables (excluding the identifier, `HH.P.id`, and the weight variable, `ww`). For this example we will use the categorical versions of the variables and one geographic area: `urb`.</p>

```{r}
varshared <- c("urb", "c.age", "hsize5", "sex", "marital", "edu7") # shared variables
```

There is also a weight variable with the same name in both files. (Note that naming the same variables equally is in general a good practice).

```{r}
weights <- "ww" # weight variable (same name in samp.A and samp.B)
```

Now that we have all the information, the purpose of matching can be made concrete: 

        * We want to relate variables `c.neti` and `labour5` by applying some matching
        method that will use a subset of `varshared` variables, and producing a synthetic, 
        complete file. Specifically, we will fill `samp.A` -which therefore will act as the 
        receptor file-, with variable `labour5` from `samp.B` -which will be the donor file
        -.

**Important Note**
In general, the file with less observations is used as receptor. The reason is that, otherwise, observations would have to be used many times in order to "fill"" the bigger file.

In `micromatch`, we have a way to express this by means of `receptor` and `donor` constructor functions:

```{r constructObjets}
library(micromatch)
# create the receptor object
rec <- receptor(data = samp.A, matchvars = varshared, specvars = varesp_A, weights=weights) # create the donor object
don <- donor(data = samp.B, matchvars = varshared, specvars = varesp_B, weights=weights)
```

We can check the parameter (slot) values by using `str`:

```{r checkValues}
str(rec)
str(don)
```

#### Step 3-1 (assess coherence) 

We now inspect the concordance of marginal distributions of the shared variables. In `micromatch` three kind of tools are implemented: frequency tables, plots and empirical measures (as computed by `comp.prop` function in `StatMatch`).

Because we have previously stored information about each type of variable we only need to choose the options, i.e. 

* `type`: equal to `table`, `plot` or `measures`; 
* `cell_values`: `abs` (absolute numbers) `rel` (relative, i.e. percents) for type `table` or `plot
* `weights`: `TRUE` or `FALSE`;
* `strata`: `TRUE` or `FALSE`

The third option may be used when a strata variable is introduced, i.e. in case we want to perform matching separately for distinct groups in the population (male and female, etc).
 
```{r}
# tables
compare_matchvars(x = rec, y = don, type = "table", cell_values = 'abs', weights = TRUE)
# plots
compare_matchvars(x = rec, y = don, type = "plot", cell_values = 'rel', weights = TRUE)
# disimilarity measures
compare_matchvars(x = rec, y = don, type = "measures", weights = TRUE)
```

Overall, for the these marginal distributions, both tables and graphics indicate that the shared variables are highly concordant. 

In particular, Hellinger's distance (Hell) is below 0.05 in all cases (an usual rule of thumb in statistical matching).

#### Step 3-2 (assess predictive value)

Now we will assess the predictive value of the common variables with respect to the specific ones, in order to discard unnecessary information.

In `micromatch`, for categorical variables we can use `predictvalue` which relies on `StatMatch` function `pw.assoc`. This function returns four well-known statistical association measures for all the combinations of variables, based on Chi-Square and others:

* Cramer's `V`
* Goodman-Kruskal `lambda`
* Goodman-Kruskal `tau`
* Theil's uncertainty coefficient `U`

For more information on these measures please refer to `StatMatch` or Agresti's book ([6]).

```{r predictValue}
predictvalue(x = rec) # predictive value in file samp.A
predictvalue(x = don) # predictive value in file samp.B
```

A simple, temptative choice would be to keep varibles `c.age`, `sex` and `edu7`. Also, it can be a good idea to introduce `sex` as a group or strata variable. 

**Note**

This variable selection is also backed by the reduction-of-uncertainty analyses in the `StatMatch` package vignette. This functionality has not been implemented in `micromatch`.

We now proceed to update the information in the `receptor` and `donor` objects by using `update` function in `micromatch`. Note that the new objects must be stored in the session:

```{r update}
rec1 <- update(x = rec, matchvars = c("c.age", "edu7"), stratavars = "sex") # update variables for file A (receptor)
don1 <- update(x = don, matchvars = c("c.age", "edu7"), stratavars = "sex") # update variables for file B (donor)
```

#### Step 4:

In this example distance hot-deck imputation will be used to fill the non-observed values (variable `labour5` from `samp.B`) in file `samp.A`. 

In `micromatch` we can use the `match.hotdek` function, which in turn calls to `NND.hotdeck` function in `StatMatch`. This function finds the closest donor record in `don` for each file in `rec`, based on the chosen matching variables, in this case, `c.age` and `edu7` (variables defined in `matchvars`). The search is made within levels of `sex`, that is, separately for male and female.

After receptor, donor pairs are formed the algorithm imputes the value observed in the donor register to the receptor register. In this way, the receptor file is 'completed'. The function `match.hotdeck` performs the two steps as a result of only one command:

```{r matchHotDeck}
result <- match.hotdeck(x = rec, y = don)
```

Now we can access the data and see the imputed values (last column, `labour5`). Note that the new data frame is stored in the session with the name `A.imputed`:

```{r storeImputedData}
samp.A.imp <- slot(result, "data")
head(samp.A.imp)
```

TODO. Details about the receptor and donor pairs can be obtained by means of the `details` function.

#### Step 5:

The first, reasonable validation is to check the concordance of imputed vs observed marginal distributions.

In our example the first validation would consist in comparing the distribution for variable `labour5` in the original file `samp.B` versus the imputed variable in `samp.A.imp` file. 

For this purpose, we can use `tabulate2cat`, `plot2cat` and `similarity2cat` functions in `micromatch`, which essentially provide the same functionality as `compare_matchvars` (seen in step 3-1). 

Note that, instead of handling special objets, here we use the data frames directly: in this case, `samp.B` and `samp.A.imp`. The variable to compare is `labour5` in both files, so, for convenience, we can store it as a character. The same happens with `ww`, the weights variable:

```{r validateFirstOrder}
var <- "labour5"
weights <- "ww"
# raw tables
tabulate2cat(data_A = samp.B, data_B = samp.A.imp, var_A = var, var_B = var, weights_A = weights, weights_B = weights, cell_values = "rel")
# plots with percents
plot2cat(data_A = samp.B, data_B = samp.A.imp, var_A = var, var_B = var, weights_A = weights, weights_B = weights, cell_values = "rel") # blue bar corresponds to imputed values
# empirical measures
similarity2cat(data_A = samp.B, data_B = samp.A.imp, var_A = var, var_B = var, weights_A = weights, weights_B = weights) 
```

The results are quite acceptable, but we should continue comparing distributions conditioned on other variables.

<p align="justify">For example, a natural comparison would be to check distributions conditioned on `sex`, which was in fact used as strata variable. This can be done in with the same functions, by iterating over strata values, as follows:</p

```{r}
levels(samp.B$sex) # codes for gender: 1-male, 2-female, check ?samp.A
# Gender equal to "1" = male
similarity2cat(data_A = subset(samp.B, sex == "1"), data_B = subset(samp.A.imp, sex == "1"), var_A = var, var_B = var, weights_A = weights, weights_B = weights)
# Gender equal to "2" = female
similarity2cat(data_A = subset(samp.B, sex == "2"), data_B = subset(samp.A.imp, sex == "2"), var_A = var, var_B = var, weights_A = weights, weights_B = weights)
```

Results seem to be 'good' by strata too.

<p align="justify">However, in statistical matching the validation should imply a lot more effort. The reason is that most matching algorithms assume what is known as the _conditional independence assumption_, which amounts to taking for granted that common variables (Z) explain (or _mediate_ between), all the (non-observed) relation between specific variables (X and Y).</p>

<p align="justify">Such an assumption is particularly strong and seldom holds in practice. What is worse, in default of complete observations —possibly in the form of a third independent file `C`, that may contain observatins for all variables, maybe from a previous wave of the same surveys, and not too distant in time so that it refers to almost the same population of interest—, we will lack of the necessary data in order to check how far we are from the ideal situation.</p>

One recommended approach is to perform an uncertainty analysis.

**Note** 

Additional functions will be soon implemented to accept iteration over strata variable groups.

Additional features
-------------------

References
==========

[1] D'Orazio, M., Di Zio, M., & Scanu, M. (2006). *Statistical matching: Theory and practice*. John Wiley & Sons.

[2] Rässler, S. (2002). *Statistical matching*. Springer.

[3] *Data Integration* ESSnet project. (http://www.cros-portal.eu/content/data-integration-finished)

[4] *ISAD* ESSnet project (http://www.cros-portal.eu/content/isad-finished)

[5] Leulescu A. & Agafitei, M. *Statistical matching: a model based approach for data integration*, Eurostat methodologies and working papers, 2013. (http://epp.eurostat.ec.europa.eu/cache/ITY_OFFPUB/KS-RA-13-020/EN/KS-RA-13-020-EN.PDF)
