---
title: "`micromatch` package:"
subtitle: "Utilities and functions for statistical matching"
author: "Ines Garmendia"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{micromatch package}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc}
---

About this document
===================

<p align="justify">This is the main vignette for `micromatch` package. This package provides a set of utilities to ease the task of statistically matching independent microdata files, with a focus to official statistics. 

The main methods in `micromatch` are described in two books (see [1] and [2]), and are a result of two Eurostat projects in Data Integration and Statistical Matching (see [3] and [4]). `micromatch` heavily relies on `StatMatch`, an R package that implements the main methods in the mentioned sources.</p>

<p align="justify">This document has two main parts. In the first chapter the reader will find an overview of the main concepts in statistical matching methodology. The second chapter deals with the use of `micromatch` in practice.</p>

<p align="justify">`micromatch` package also provides vignettes for specific examples with real data. These additional documents can be found in the package documentation.</p>

Fundamentals of Statistical Matching
====================================

> Statistical matching provides a methodology to explore ways for producing combined analyses or indicators for independent surveys referred to the same population of interest, from data with to distinct observations and stored in separate files, but sharing a common block of information

<p align="justify"><strong>Statistical matching</strong> (also known as data fusion, data merging or synthetic matching) is set of techniques for providing joint information on variables or indicators collected through multiple sources, usually, surveys drawn from the same population of interest. The potential benefits of this approach lie in the possibility to enhance the complementary use and analytical potential of existing data sources. ([5] A. Leulescu & M. Agafitei, 2013).</p>

<p align="justify">Statistical matching has been widely used in market research, to link consumer behavior data and media consumption data.</p>

<p align="justify">In official statistics, it can be used to link different aspects that are usually studied separately for the same target population (i.e. the inhabitants in a country or a particular geographic area). A unique questionnaire covering all aspects such as population health, income, consumption, labour market, social capital... is seldom conceived: such a questionnaire would be too long, leading to a higher response burden, and to poor quality.</p>

<p align="justify">A separate survey is usually conducted to study each specific aspect of the population, the drawback being that the responses will eventually lie in separate files. Statistical matching tries to overcome this limitation, by making use of shared information in order to infer some type of "new" knowledge about aspects measured through independent surveys.</p>

The starting point (the input)
------------------------------

> The basic assumption is that the number of individuals or units in both samples (i.e., the overlap) is negligible. In fact, the fundamental difference with respect to other methods such as "record linkage" is that in the latter, we have identical units and we wish to find a correspondence between them to link the files. 
In statistical matching, we "know" the units are different, but we "wish" to find similar ones.

<p align="justify">Consider two independent surveys conducted on the same population of interest, each of which produces measures regarding a specific field (e.g. living styles and consumer behavior).</p> 

<p align="justify">The surveys share a block of variables (sociodemographic variables such as the age, sex, or social status). When putting observations from distinct sources together, a particular missing data pattern emerges due to the non-observed values (i.e. answers we don't have in one survey just because they correspond to the other, and viceversa), see Fig 1.</p>

<div align="center"><img src="./fig1.png"><figcaption>Fig 1. The starting point: a block of common variables (Z) and two block of specific, non-jointly-observed, variables (X and Y)</figcaption></div>

<p align="justify">The aim is to obtain integrated analyses or results relating the non-jointly-observed variables (blocks X and Y in the figure), and to achieve this we need to make use of the common information between the files (block Z) in some efficient and reliable way.</p>

The results (the output)
------------------------

<p align="justify">After matching will typically obtain one of these results:</p>

* a synthetic file containing full information on the variables and all units from both sources. This enhanced dataset can be used later to make combined statistical analyses.

* particular estimates regarding variables living in separate files. The user might wish to estimate a contingency table or correlation coefficient, or any parameter of interest regarding variables in separate files. 

The former is named the **micro** approach. The latter is the **macro** approach.

The matching process
--------------------

<p align="justify">Regardless of the matching method itself —that is, the computational method by means of which we will produce a synthetic file (in the micro case) or estimations for certain parameters of the joint distributions (in the macro case)—, the matching task involves a series of <strong>pre-processing steps</strong> that have to be tackled in practice:</p>

1. The choice of target variables (X and Y), i.e. the variables observed separately in distinct surveys.

2. Identification of the variables shared by the sources, and the study of their degree of coherence taking into account not only the wording of questions (which can be different, leading to non-agreeable measures), but also the marginal distributions observed separately in the data files. Variables that fail to show a minimum degree of coherence must be discarded. This step can be time-consuming but can also the key for a successful matching. 

3. Possibly, discarding further variables that are not predictive (i.e. are not related to) for the target variables, o are redundant with others. This can be of a relevant step, depending on the computational method. 

4. The choice of a matching framework (parametric, non-parametric, mixed) in a specific setting (micro or macro), and applying the corresponding matching/imputation/estimation algorithm. This algorithm will make use of the chosen subset of shared variables in steps 2 and 3 (namely, the _common matching variables_) to relate target variables fixed in step 1 (the _specific variables_).

5. A thorough validation of results.

Using `micromatch`
=================

        * The idea is that the user should start defining particular attributes of the 
        data files related to the statistical matching context, i.e. common and specific 
        variables, weights, and so on. In this way, each step in the matching process 
        has its implementation (or definition) by means of functions or methods that 
        "act" on these special objects.

<p align="justify">This chapter deals with using `micromatch` in practice, and shows how functions in this package may be used to tackle a specific matching task with real data.</p>

<p align="justify">Well-known `R` packages such as `StatMatch` or `mice` provide sophisticated algorithms to solve different statistical matching tasks. `micromatch` does not offer new algorithms for matching; rather, it provides a "context" where the matching task is made easier, independently of the chosen methodology. In this way, alternative methodologies available through independent packages are integrated in a common "matching pipeline".</p>

<p align="justify">From the programming point of view, `micromatch` makes use of S4 classes to create special objects which are adapted to the statistical matching task.</p>

A simple example
----------------

<p align="justify">To illustrate the use of `micromatch` we will be using data frames data `samp.A` and `samp.B` included in `StatMatch` package. These examples provide some artificial data simulating typical variables present in the European Union Statistics on Income and Living Conditions Survey (EU-SILC).</p>

```{r loadStatMatch, warning=FALSE, message=FALSE}
library(StatMatch)
data(samp.A) #loads data into workspace
data(samp.B) #loads data to workspace
str(samp.A)
str(samp.B)
```

The independent sources `samp.A` and `samp.B`, separately contain:

* a shared block of variables: 

    + `HH.P.id`: unit identifier
    + `area5` and `urb`: geographic variables
    + `hsize` and `hsize5`: family size (numeric and categorized)
    + `age` and `c.age`: age (numeric and categorized)
    + `sex`: gender
    + `marital`: marital status
    + `edu7`: education level

* one specific variable in each of the files:

    + in file `samp.A`: `n.income` and `c.neti`, net personal income (numeric and categorized, thousand of euros)
    
    + in file `samp.B`: `labour5`, the person's self-defined economic status.

* a weight variable,    `ww`, with the same name in both files

For more information on these data files please refer to `StatMatch` package documentation.

Now we will illustrate how the matching task can be tackled with `micromatch`, step by step.

#### Step 1: 

The specific (target) variables are the income and the labour status, and it is advisable to store their name in the `R` session. 

For this example we will use the categorical version of variable income, `c.neti`:

```{r}
varesp_A <- "c.neti" # specific variable in file samp.A
varesp_B <- "labour5" # specific variable in file samp.B
```

#### Step 2: 

<p align="justify">The shared variables are the remaining variables (excluding the identifier, `HH.P.id`, and the weight variable, `ww`). For this example we will use the categorical versions of the variables and one geographic area: `urb`.</p>

```{r}
varshared <- c("urb", "c.age", "hsize5", "sex", "marital", "edu7") # shared variables
```

<p align="justify">There is also a weight variable, with the same name in both files. Note that naming the same variables equally is in general a good practice.</p>

```{r}
weights <- "ww" # weight variable (same name in samp.A and samp.B)
```

Now that we have all the information, the purpose of matching can be made concrete: 

        * We want to relate variables `c.neti` and `labour5` by applying some matching
        method that will use a subset of `varshared` variables to produce a synthetic, 
        complete file. Specifically, we will fill `samp.A` -the receptor file-, by adding
        variable `labour5` from `samp.B` -the donor file-.

**Important Note**
<p align="justify">In general, the file with less observations will be used as receptor. Otherwise, donor observations would have to be used many times to "fill"" the bigger file.</p>

In `micromatch`, we have a way to assign these roles to the original data frames thanks to the `receptor` and `donor` constructor functions. 

We may also want to fill both files. In this case we will not assign any specific role to the files, and we may say they have a `symmetric` role.

In either case, the first thing in `micromatch` will be to create `receptor` and `donor` pairs (or two `symmetric` objects, not shown here):

```{r constructObjets, message=FALSE}
library(micromatch)
# create the receptor object
rec <- receptor(data = samp.A, matchvars = varshared, specvars = varesp_A, weights=weights)
#
# create the donor object
don <- donor(data = samp.B, matchvars = varshared, specvars = varesp_B, weights=weights)
```

Parameter (slot) values can be checked by using `str` function:

```{r checkValues}
str(rec)
str(don)
```

#### Step 3-1 (assess coherence) 

<p align="justify">First we must inspect the concordance of marginal distributions of the shared variables. In `micromatch` three kind of tools are implemented: frequency tables, plots and empirical measures (as computed by `comp.prop` function in `StatMatch`).</p>

<p align="justify">Because we have previously stored information about each type of variable in `receptor` and `donor` objects, all we need is to choose some options in the method `compare_matchvars`:</p>

* `type`: equal to `table`, `plot` or `measures`; 
* `cell_values`: `abs` (absolute numbers) `rel` (relative, i.e. percents) for type `table` or `plot`
* `weights`: `TRUE` or `FALSE`;
* `strata`: `TRUE` or `FALSE`: to be used when we want to study distributions separately for specific groups in the population (male and female, etc)
 
```{r}
# tables
compare_matchvars(x = rec, y = don, type = "table", cell_values = 'abs', weights = TRUE)
# plots
compare_matchvars(x = rec, y = don, type = "plot", cell_values = 'rel', weights = TRUE)
# disimilarity measures
compare_matchvars(x = rec, y = don, type = "measures", weights = TRUE)
```

<p align="justify">Overall, for the these marginal distributions, the previous results indicate that the shared variables are highly concordant between the data frames.</p>

Note that four types of empirical measures are used:

* Dissimilarity index or total variation distance, `D`

* Overlap between two distributions, `O`

* Bhattacharyya coefficient, `B`

* Hellinger's distance, `d_H`

For more information on these measures please refer to `StatMatch` or Agresti's book ([6]).

<p align="justify">In the example, Hellinger's distance (Hell) is below 0.05 in all cases (an usual rule of thumb in statistical matching, see reference [5]).</p>

#### Step 3-2 (assess predictive value)

<p align="justify">Now we should assess the predictive value of the common variables with respect to the specific ones, in order to discard unnecessary information (i.e. variables that are not predictive).</p>

<p align="justify">In `micromatch`, we can use `predictvalue` which relies on `StatMatch` function `pw.assoc`. This function returns four well-known statistical association measures for all the combinations of variables, based on Chi-Square and others. (Note that currently `predictvalue` only accepts categorical variables)</p>

* Cramer's `V`

* Goodman-Kruskal `lambda`

* Goodman-Kruskal `tau`

* Theil's uncertainty coefficient `U`

For more information on these measures please refer to `StatMatch` or Agresti's book ([6]).

```{r predictValue}
predictvalue(x = rec) # predictive value in file samp.A
predictvalue(x = don) # predictive value in file samp.B
```

A simple, temptative choice would be to keep varibles `c.age`, `sex` and `edu7`. Also, it can be a good idea to introduce `sex` as a group or strata variable. 

**Note**

<p align="justify">This variable selection is also backed by the reduction of uncertainty approach illustrated in the `StatMatch` package vignette. This functionality has not been implemented yet in `micromatch`.</p>

<p align="justify">We now proceed to update the information in the `receptor` and `donor` objects by using `update` function in `micromatch`. This function allows to re-define the objects by just updating the needed information (and keeping the rest of values unchanged).</p>

Note that the new objects must be stored in the session:

```{r update}
# update variables for file A (receptor)
rec1 <- update(x = rec, matchvars = c("c.age", "edu7"), stratavars = "sex") 
#
# update variables for file B (donor)
don1 <- update(x = don, matchvars = c("c.age", "edu7"), stratavars = "sex") 
```

#### Step 4:

<p align="justify">In this example distance hot-deck imputation will be used to fill the non-observed values (variable `labour5` from `samp.B`) in file `samp.A`.</p>

<p align="justify">In `micromatch` we can use the `match.hotdek` function, which in turn calls to `NND.hotdeck` function in `StatMatch`. This function finds the closest donor record in `donor` for each record in `receptor`, based on the chosen matching variables.</p>

<p align="justify">In this case, `c.age` and `edu7` will be used to find similar donors, these being the variables stored as `matchvars`. We also need to indicate that the strata variable in `stratavars`, i.e. `sex` should be used as such. That is, we want the search to be made within levels of `sex`, i.e., separately for male and female:</p>

```{r matchHotDeck, message=FALSE}
# hot-deck distance matching
result <- match.hotdeck(x = rec1, y = don1, strata = TRUE)
```

This functions inherits other options available in the original function (`NND.hotdeck` in `StatMatch`). The most important are `dist.fun` and `constr.alg`:

* dist.fun: Choice of distance function. Available options are “Manhattan” (aka “City block”; default), “Euclidean”, “Mahalanobis”,“exact” or “exact matching”, “Gower”, “minimax” or one of the distance functions available in `proxy` package. For more information check `?NND.hotdeck`

* constr.alg: `TRUE` or `FALSE`. Indicates if the algorithm should be constrined, i.e. donor records should be used only once to fill the receptor records.

<p align="justify">The procedure has two main steps. First (receptor, donor) pairs are formed which are similar in terms of `matchvars`. Second, value observed in the donor record to is picked to fill the receptor record in each pair. In this way, the `receptor` file is 'completed'. The function returns an object of type `fusedfile`, in which `receptor` data are stored with additional (imputed) columns (in the example, an unique column, `labour5`).</p>

<p align="justify">The completed data can be re-used for further computations by extracting and storing the data frame in the session, as follows. (In the example our case we store the new data contains the additional column `labour5` and is stored with the name `A.imputed`):</p>

```{r storeImputedData}
# Extract the new, 'complete' data and store it under the name 'A.imputed'
samp.A.imp <- slot(result, "data")
#
# First 6 records. 
# The last column contains the imputed values for variable 'labour5'.
head(samp.A.imp)
```

TODO. Details about the receptor and donor pairs can be obtained by means of the `details` function.

#### Step 5:

Now we should assess the validity of the resulting data frame, in terms of its usefulness to perform statistical analyses relating not-jointly observed variables (in our case, person's net income, `c.neti` and self-defined labour status, `labour5`).

The first, reasonable validation should be to check the similarity of imputed versus observed marginal distributions. For this purpose, we can use `tabulate2cat`, `plot2cat` and `similarity2cat` functions in `micromatch`, which essentially provide the same functionality as `compare_matchvars` (see Step 3-1 above). 

In our example the distribution for variable `labour5` in the original file `samp.B` whould be compared to the imputed variable in `samp.A.imp` file. In `tabulate2cat`, `plot2cat` and `similarity2cat` functions, data frames have to be introduced directly as parameter values: in the example, `samp.B` and `samp.A.imp`.

* TODO. create `validate1` method that will act on rec.fused, don pairs with options type=table, plot or measures.

The variable to be compared is `labour5` in both files. The distributions are based on weighted data i.e. using the weights variable `ww`:

```{r validateFirstOrder}
# Comparison of imputed vs observed distribution for variable 'labour5'
#
# store names in the session (for convenience)
var <- "labour5"
weights <- "ww"
#
# Compute raw tables
tabulate2cat(data_A = samp.B, data_B = samp.A.imp, var_A = var, var_B = var, weights_A = weights, weights_B = weights, cell_values = "rel")
#
# Plots with percents
plot2cat(data_A = samp.B, data_B = samp.A.imp, var_A = var, var_B = var, weights_A = weights, weights_B = weights, cell_values = "rel") # blue bar corresponds to imputed values
#
# Empirical measures
similarity2cat(data_A = samp.B, data_B = samp.A.imp, var_A = var, var_B = var, weights_A = weights, weights_B = weights) 
```

<p align="justify">The results are quite acceptable, but we should also compare distributions conditioned on other variables common to both files.</p>

<p align="justify">For example, a natural comparison would be to check distributions conditioned on `sex`, which was in fact used as strata variable. This can be done in with the same functions, by subseting over strata values, as follows:</p

```{r}
levels(samp.B$sex) # codes for gender: 1-male, 2-female, check ?samp.A
#
# Gender equal to "1" = male
similarity2cat(data_A = subset(samp.B, sex == "1"), data_B = subset(samp.A.imp, sex == "1"), var_A = var, var_B = var, weights_A = weights, weights_B = weights)
#
# Gender equal to "2" = female
similarity2cat(data_A = subset(samp.B, sex == "2"), data_B = subset(samp.A.imp, sex == "2"), var_A = var, var_B = var, weights_A = weights, weights_B = weights)
```

Results seem to be 'good' by strata too.

*TODO. implement option `strata` true, false in `validate1` method.

<p align="justify">However, in statistical matching the validation should imply a bit more effort. The reason is that most matching algorithms assume what is known as the <em>conditional independence assumption</em>, which amounts to saying that the common variables (Z) explain (or "mediate between") all the (non-observed) relation between specific variables (X and Y).</p>

<p align="justify">Such an assumption is particularly strong and seldom holds in practice. What is worse, in abscence of complete observations —possibly in the form of a third independent file `C`, that may contain observatins for all variables, maybe from a previous wave of the same surveys, and not too distant in time so that it refers to almost the same population of interest—, we will lack of the necessary information to check how far we are from the ideal situation.</p>

One recommended approach is to perform an <em>uncertainty analysis</em>. In the case of categorical variables, the ultimate aim will is to estimate contingency tables between variables observed in separate files, and <em>Frèchet bounds</em> can be computed to compute a range of possible vaues, i.e. numeric results that are coherent with the independently observed marginal distributions (i.e. `X` versus `Z` and `Y` versus `Z`). For more information Frèchet bounds please refer to [1] or to the `StatMatch` package vignette.</p>

* TODO. Here, an illustration of uncertainty in cell values in contingency table between net income and labour status

```{r uncertainty}
# Uncertainty if we use all shared variables
slot(rec, "matchvars") #"urb"     "c.age"   "hsize5"  "sex"     "marital" "edu7" 
xx <- xtabs(~urb + c.age + hsize5 + sex + marital + edu7, data = samp.B)
xy <- xtabs(~urb + c.age + hsize5 + sex + marital + edu7 + c.neti, data = samp.A)
xz <- xtabs(~urb + c.age + hsize5 + sex + marital + edu7 + labour5, data = samp.B)
#todas las posibles combinaciones de vars comunes
#Fbwidths.by.x(tab.x = xx, tab.xy = xy, tab.xz = xz) 
Frechet.bounds.cat(tab.x = xx, tab.xy = xy, tab.xz = xz, print.f = "data.frame")
```




Additional features
-------------------

References
==========

[1] D'Orazio, M., Di Zio, M., & Scanu, M. (2006). *Statistical matching: Theory and practice*. John Wiley & Sons.

[2] Rässler, S. (2002). *Statistical matching*. Springer.

[3] *Data Integration* ESSnet project. (http://www.cros-portal.eu/content/data-integration-finished)

[4] *ISAD* ESSnet project (http://www.cros-portal.eu/content/isad-finished)

[5] Leulescu A. & Agafitei, M. *Statistical matching: a model based approach for data integration*, Eurostat methodologies and working papers, 2013. (http://epp.eurostat.ec.europa.eu/cache/ITY_OFFPUB/KS-RA-13-020/EN/KS-RA-13-020-EN.PDF)
